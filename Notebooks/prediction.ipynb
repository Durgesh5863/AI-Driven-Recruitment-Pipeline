{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Durgesh Babu\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "import textstat\n",
    "from textblob import TextBlob\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_data = pd.read_excel(\"Copy of prediction_data.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorizer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tfidf_vectorizer.pkl', 'rb') as f:\n",
    "    tfidf_vectorizer = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentence Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the SentenceTransformer model\n",
    "sentence_transformer = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved model\n",
    "with open('sentence_transformer_model.pkl', 'rb') as file:\n",
    "    classifier = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I need 18 features\n",
    "\n",
    "'num_words_in_transcript',\n",
    "    'resume_jd_similarity', 'resume_transcript_similarity', 'sentiment',\n",
    "    'transcript_length', 'resume_length',\n",
    "    'job_description_experience_match', 'text_complexity_transcript',\n",
    "    'text_complexity_resume', 'lexical_diversity', \n",
    "    'technical_skill_match', 'soft_skills_sentiment',\n",
    "    'cultural_fit_sentiment', 'job_fit_score', 'confidence_score',\n",
    "    'clarity_score', 'job_desc_complexity', 'interaction_quality'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate resume and job description similarity (Cosine Similarity)\n",
    "vectorizer = TfidfVectorizer()\n",
    "resume_jd_similarity = []\n",
    "for i in range(len(pred_data)):\n",
    "    resume = pred_data['Resume'][i]\n",
    "    jd = pred_data['Job Description'][i]\n",
    "    similarity = cosine_similarity(tfidf_vectorizer.transform([resume, jd]))[0, 1]\n",
    "    resume_jd_similarity.append(similarity)\n",
    "pred_data['resume_jd_similarity'] = resume_jd_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate resume and transcript similarity (Cosine Similarity)\n",
    "resume_transcript_similarity = []\n",
    "for i in range(len(pred_data)):\n",
    "    resume = pred_data['Resume'][i]\n",
    "    transcript = pred_data['Transcript'][i]\n",
    "    similarity = cosine_similarity(tfidf_vectorizer.transform([resume, transcript]))[0, 1]\n",
    "    resume_transcript_similarity.append(similarity)\n",
    "pred_data['resume_transcript_similarity'] = resume_transcript_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2. Sentiment Polarity Distribution:\n",
      "   - Positive: 100 occurrences (100.00%)\n",
      "\n",
      "3. Overall Average Sentiment Score: 1.00\n",
      "   - The overall sentiment of the transcripts is positive.\n"
     ]
    }
   ],
   "source": [
    "# Initialize VADER sentiment analyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Lists to store sentiment results\n",
    "sentiments = []\n",
    "polarity = []\n",
    "\n",
    "# Perform sentiment analysis on each transcript\n",
    "for i in range(len(pred_data)):\n",
    "    transcript = pred_data['Transcript'][i]\n",
    "    sentiment_score = sia.polarity_scores(transcript)\n",
    "    sentiments.append(sentiment_score['compound'])  # Compound sentiment score\n",
    "    polarity.append('positive' if sentiment_score['compound'] > 0 \n",
    "                    else 'negative' if sentiment_score['compound'] < 0 \n",
    "                    else 'neutral')\n",
    "\n",
    "# Add the results to the DataFrame\n",
    "pred_data['sentiment'] = sentiments\n",
    "pred_data['polarity'] = polarity\n",
    "\n",
    "# Count of each sentiment category\n",
    "polarity_counts = pred_data['polarity'].value_counts()\n",
    "print(\"\\n2. Sentiment Polarity Distribution:\")\n",
    "for polarity, count in polarity_counts.items():\n",
    "    print(f\"   - {polarity.capitalize()}: {count} occurrences ({(count / len(pred_data) * 100):.2f}%)\")\n",
    "\n",
    "# Overall average sentiment score\n",
    "average_sentiment = pred_data['sentiment'].mean()\n",
    "print(f\"\\n3. Overall Average Sentiment Score: {average_sentiment:.2f}\")\n",
    "if average_sentiment > 0:\n",
    "    print(\"   - The overall sentiment of the transcripts is positive.\")\n",
    "elif average_sentiment < 0:\n",
    "    print(\"   - The overall sentiment of the transcripts is negative.\")\n",
    "else:\n",
    "    print(\"   - The overall sentiment of the transcripts is neutral.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate lexical diversity\n",
    "def lexical_diversity(text):\n",
    "    words = text.split()\n",
    "    return len(set(words)) / len(words)\n",
    "\n",
    "# Compute lexical diversity for each transcript\n",
    "pred_data['lexical_diversity'] = pred_data['Transcript'].apply(lexical_diversity)\n",
    "\n",
    "# Calculate statistics\n",
    "average_diversity = pred_data['lexical_diversity'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of transcript (number of words)\n",
    "pred_data['transcript_length'] = pred_data['Transcript'].apply(lambda x: len(x.split()))\n",
    "\n",
    "# Calculate statistics\n",
    "average_length = pred_data['transcript_length'].mean()\n",
    "min_length = pred_data['transcript_length'].min()\n",
    "max_length = pred_data['transcript_length'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute similarity score between Resume and Job Description\n",
    "def compute_similarity(text1, text2):\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    tfidf_matrix = tfidf_vectorizer.transform([text1, text2])\n",
    "    return cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]\n",
    "\n",
    "# Calculate technical skill matching score\n",
    "pred_data['technical_skill_match'] = pred_data.apply(lambda row: compute_similarity(row['Resume'], row['Job Description']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Soft Skills\n",
    "pred_data['soft_skills_sentiment'] = pred_data['Transcript'].apply(lambda x: TextBlob(x).sentiment.polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resume length (number of words)\n",
    "pred_data['resume_length'] = pred_data['Resume'].apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Job Description Experience Match (Simple matching based on keywords, could be improved)\n",
    "pred_data['job_description_experience_match'] = pred_data.apply(lambda row: len(set(row['Resume'].split()) & set(row['Job Description'].split())), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cultural fit sentiment\n",
    "pred_data['cultural_fit_sentiment'] = pred_data['Reason for decision'].apply(lambda x: TextBlob(x).sentiment.polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#job score\n",
    "def job_fit_analysis(job_desc, transcript):\n",
    "    # You can use similarity or keyword matching here\n",
    "    job_keywords = job_desc.split()\n",
    "    transcript_keywords = transcript.split()\n",
    "    common_keywords = set(job_keywords).intersection(transcript_keywords)\n",
    "    return len(common_keywords) / len(job_keywords)\n",
    "\n",
    "pred_data['job_fit_score'] = pred_data.apply(lambda row: job_fit_analysis(row['Job Description'], row['Transcript']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Define a function to calculate the confidence score\n",
    "def calculate_confidence_score(text):\n",
    "    # Count occurrences of \"I think\" and \"Maybe\" (case-insensitive)\n",
    "    confidence_phrases = re.findall(r'\\b(I think|Maybe)\\b', text, flags=re.IGNORECASE)\n",
    "    return len(confidence_phrases)\n",
    "\n",
    "# Apply the function to calculate confidence scores\n",
    "pred_data['confidence_score'] = pred_data['Transcript'].apply(calculate_confidence_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#job description complexity\n",
    "pred_data['job_desc_complexity'] = pred_data['Job Description'].apply(lambda x: textstat.flesch_reading_ease(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#interaction quality\n",
    "pred_data['interaction_quality'] = pred_data['num_words_in_transcript'] * pred_data['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_data['clarity_score'] = pred_data['Transcript'].apply(lambda x: textstat.flesch_reading_ease(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text complexity (resume and transcript - using a simple metric like Flesch Reading Ease)\n",
    "def text_complexity(text):\n",
    "    # Implement text complexity (e.g., Flesch Reading Ease)\n",
    "    # Here's a placeholder function:\n",
    "    return len(text.split()) / len(set(text.split()))  # A basic metric\n",
    "\n",
    "pred_data['text_complexity_transcript'] = pred_data['Transcript'].apply(text_complexity)\n",
    "pred_data['text_complexity_resume'] = pred_data['Resume'].apply(text_complexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0.1', 'Unnamed: 0', 'ID', 'Name', 'Role', 'Transcript',\n",
       "       'Resume', 'Reason for decision', 'Job Description',\n",
       "       'num_words_in_transcript', 'resume_jd_similarity',\n",
       "       'resume_transcript_similarity', 'sentiment', 'polarity',\n",
       "       'lexical_diversity', 'transcript_length', 'technical_skill_match',\n",
       "       'soft_skills_sentiment', 'resume_length',\n",
       "       'job_description_experience_match', 'cultural_fit_sentiment',\n",
       "       'job_fit_score', 'confidence_score', 'job_desc_complexity',\n",
       "       'interaction_quality', 'clarity_score', 'text_complexity_transcript',\n",
       "       'text_complexity_resume'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to convert the text into embeddings\n",
    "def get_embeddings(texts, model):\n",
    "    return np.array([model.encode(text) for text in texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get embeddings for 'Transcript', 'Job Description', and 'Resume'\n",
    "transcript_embeddings = get_embeddings(pred_data['Transcript'], sentence_transformer)\n",
    "job_desc_embeddings = get_embeddings(pred_data['Job Description'], sentence_transformer)\n",
    "resume_embeddings = get_embeddings(pred_data['Resume'], sentence_transformer)\n",
    "reason_embeddings = get_embeddings(pred_data['Reason for decision'], sentence_transformer)\n",
    "polarity_embeddings = get_embeddings(pred_data['polarity'], sentence_transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features_array = pred_data[\n",
    "    [\n",
    "        'num_words_in_transcript', 'resume_jd_similarity', \n",
    "        'resume_transcript_similarity', 'sentiment',\n",
    "        'lexical_diversity', 'transcript_length', 'technical_skill_match',\n",
    "        'soft_skills_sentiment', 'resume_length',\n",
    "        'job_description_experience_match', 'cultural_fit_sentiment',\n",
    "        'job_fit_score', 'confidence_score', 'job_desc_complexity',\n",
    "        'interaction_quality', 'clarity_score', \n",
    "        'text_complexity_transcript', 'text_complexity_resume'\n",
    "    ]\n",
    "].to_numpy()\n",
    "\n",
    "\n",
    "# Concatenate the embeddings\n",
    "features = np.concatenate([transcript_embeddings, resume_embeddings, reason_embeddings, job_desc_embeddings, polarity_embeddings, num_features_array], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict decision using embedding features\n",
    "embed_decision = classifier.predict(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted decision: [1 0 0 1 1 1 0 1 1 1 0 1 1 0 1 0 0 0 0 1 1 0 0 0 1 0 0 1 0 0 1 1 1 1 0 0 0\n",
      " 1 0 1 0 1 0 1 0 1 0 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 0 1 1 0 1 0 1 0 0 1 0 1\n",
      " 0 0 1 0 0 0 1 1 1 0 1 1 0 1 1 1 0 0 1 1 1 1 0 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "print(\"Predicted decision:\", embed_decision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add predictions to the DataFrame\n",
    "pred_data['Decision'] = ['Accept' if pred == 1 else 'Reject' for pred in embed_decision]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_data.to_csv('prediction.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Send Mail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import smtplib\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "from email.mime.text import MIMEText\n",
    "from email.mime.base import MIMEBase\n",
    "from email import encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Email sent successfully!\n"
     ]
    }
   ],
   "source": [
    "# Define a function to send an email\n",
    "def send_email_with_attachment(sender_email, sender_password, to_email, subject, body, file_path):\n",
    "    try:\n",
    "        # Create the email message\n",
    "        message = MIMEMultipart()\n",
    "        message[\"From\"] = sender_email\n",
    "        message[\"To\"] = to_email\n",
    "        message[\"Subject\"] = subject\n",
    "        message.attach(MIMEText(body, \"plain\"))\n",
    "\n",
    "        # Attach the file\n",
    "        if file_path and os.path.exists(file_path):\n",
    "            with open(file_path, \"rb\") as attachment:\n",
    "                part = MIMEBase(\"application\", \"octet-stream\")\n",
    "                part.set_payload(attachment.read())\n",
    "            encoders.encode_base64(part)\n",
    "            part.add_header(\n",
    "                \"Content-Disposition\",\n",
    "                f\"attachment; filename={os.path.basename(file_path)}\",\n",
    "            )\n",
    "            message.attach(part)\n",
    "        else:\n",
    "            print(f\"File not found: {file_path}\")\n",
    "\n",
    "        # Send the email\n",
    "        with smtplib.SMTP(\"smtp.gmail.com\", 587) as server:\n",
    "            server.starttls()  # Start TLS encryption\n",
    "            server.login(sender_email, sender_password)  # Log in to the email server\n",
    "            server.send_message(message)  # Send the email\n",
    "            print(\"Email sent successfully!\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to send email: {e}\")\n",
    "\n",
    "# Email details\n",
    "sender_email = \"durgeshbabu5863@gmail.com\"\n",
    "sender_password = \"bdtc cjfu vvro afdx\"  # App-specific password\n",
    "to_email = \"malathula00000@gmail.com\"\n",
    "subject = \"Predicted Outcome of the Candidates\"\n",
    "body = \"Here is the attached pdf which consists of the predicted outcome of the candidates\"\n",
    "file_path = r\"D:\\Internship - Infosys\\Project\\prediction.csv\"\n",
    "\n",
    "# Call the function to send the email\n",
    "send_email_with_attachment(sender_email, sender_password, to_email, subject, body, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
